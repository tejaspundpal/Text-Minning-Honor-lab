Write a Python program to generate word embedding for documents corpus using Word2vec algorithm. 


import nltk
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec

# Example document corpus
documents = [
    "I like to eat pizza",
    "Pizza is my favorite food",
    "I enjoy eating pizza with friends"
]

# Check if the 'punkt' tokenizer is available
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    # Download the tokenizer if not found
    nltk.download('punkt')

# Tokenize the documents into individual words
tokenized_docs = [word_tokenize(doc.lower()) for doc in documents]

# Train the Word2Vec model on the tokenized documents
model = Word2Vec(tokenized_docs, size=100, window=5, min_count=1, workers=4)

# Get the word embeddings for a specific word
word = "pizza"
embedding = model.wv[word]

# Print the word embedding vector
print(f"Word: {word}")
print(f"Embedding: {embedding}")


output:-

Word: pizza
Embedding: [ 0.00677784 -0.00347636 -0.00278313  0.00056076 -0.00208497  0.00147725
 -0.00423796  0.00116968  0.00374721 -0.00303478 -0.00072281  0.00240766
 -0.0036045  -0.00323897  0.00061235 -0.00200923 -0.00024791  0.00430308
 -0.00093892 -0.00363414  0.00138811 -0.00194696 -0.00424529  0.00141194
 -0.00096157 -0.00208645 -0.00309292 -0.00078375 -0.00428014 -0.00025507
  0.00200841 -0.00415489 -0.00326336  0.00302857 -0.00331668 -0.00125363
 -0.00178344  0.00201179  0.00264357 -0.00424218 -0.00126428  0.00169605
 -0.00091195 -0.0010713  -0.0040522   0.00429726 -0.00178002 -0.00297392
 -0.00353222  0.00107571 -0.00339684 -0.00221971 -0.00054224 -0.00133392
  0.00403847 -0.00360075 -0.00036462 -0.00136919 -0.00417775 -0.00378413
  0.00307804  0.0008197   0.0019434   0.00307125  0.00198866  0.00295106
  0.00252286 -0.0007309   0.00152632 -0.00169502 -0.00340832 -0.00351547
 -0.0026464   0.0028346   0.00089824  0.00439677 -0.00261442 -0.00335946
  0.0030623  -0.00182921 -0.00391905  0.00192901 -0.00403219  0.00167907
  0.00332671  0.00100435  0.00422555  0.00436557 -0.00251834  0.00080975
  0.0008032   0.00427701  0.00364237  0.00024971 -0.00193262  0.00062116
 -0.00417244  0.00297221  0.00323516 -0.00366154]